kind: live
title: demo-oss-dogs
id: mlops_demo_oss_dogs

defaults:
  preset: cpu-medium
  life_span: 1d
  env:
    # Those env vars will be assigned to every job in this flow
    PACHY_URI: pachyderm.green-hgx-2.org.neu.ro:30650
    PACHY_REPO: dogs-demo
    PROJECT_GIT_REPO: git@github.com:neuro-inc/mlops-demo-oss-dogs.git
    PROJECT_GIT_BRANCH: ${{ git.branch }}

volumes:
  remote_dataset:
    remote: storage:/$[[ project.owner ]]/$[[ flow.flow_id ]]/dataset
    mount: /dataset
  src:
    remote: storage:/$[[ project.owner ]]/$[[ flow.flow_id ]]/src
    mount: /usr/project/src
    local: src
  config:
    remote: storage:/$[[ project.owner ]]/$[[ flow.flow_id ]]/config
    mount: /usr/project/config
    local: config
  notebooks:
    remote: storage:/$[[ project.owner ]]/$[[ flow.flow_id ]]/notebooks
    mount: /usr/project/notebooks
    local: notebooks

images:
  train:
    ref: image:/$[[ project.owner ]]/$[[ flow.flow_id ]]/train:21.5.13-pachyderm
    dockerfile: $[[ flow.workspace ]]/Dockerfile
    context: $[[ flow.workspace ]]/
    build_preset: cpu-medium
  seldon:
    ref: image:/$[[ project.owner ]]/$[[ flow.flow_id ]]/seldon:20.12.16
    dockerfile: $[[ flow.workspace ]]/seldon/seldon.Dockerfile
    context: $[[ flow.workspace ]]/
    build_preset: cpu-medium
  label_studio:
    ref: image:/$[[ project.owner ]]/$[[ flow.flow_id ]]/label_studio:21.1.18-pachyderm
    dockerfile: $[[ flow.workspace ]]/label_studio/label_studio.Dockerfile
    context: $[[ flow.workspace ]]/label_studio
    build_preset: cpu-medium

jobs:
  create_pipeline:
    name: $[[ flow.title ]]-create-pipeline
    image: $[[ images.train.ref ]]
    volumes:
      - secret:/$[[ project.owner ]]/gh-rsa:/root/.ssh/id-rsa
    params:
      mlflow_storage:
        descr: Storage path, where MLFlow server stores trained model binaries
      mlflow_uri:
        descr: MLFlow server URI
      pachy_pipeline_name:
        default: train-dogs
        descr: MLFlow server URI
    env:
      EXPOSE_SSH: "yes"
      PYTHONPATH: /usr/project
      PROJECT: /usr/project
      MLFLOW_STORAGE: ${{ params.mlflow_storage }}
      MLFLOW_URI: ${{ params.mlflow_uri }}
      NEURO_PASSED_CONFIG: secret:platform-config
      TRAIN_IMAGE_REF: ${{ images.train.ref }}
      PACHY_PIPELINE_NAME: ${{ params.pachy_pipeline_name }}
    bash: |
      echo "IdentityFile ~/.ssh/id-rsa" > ~/.ssh/config
      ssh-keyscan github.com 2>/dev/null >> ~/.ssh/known_hosts
      git clone -b ${PROJECT_GIT_BRANCH} ${PROJECT_GIT_REPO} ${PROJECT}

      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl config set active-context default

      # Store Neuro-flow batch and project definition base-64 encoded
      BATCH_B64=$(cat ${PROJECT}/.neuro/training.yaml | base64 -w 0)
      PROJECT_B64=$(cat ${PROJECT}/.neuro/project.yml | base64 -w 0)

      # Substitue Neuro-flow batch parameters from job ENV vars and store them b64 encoded
      PARAMS_B64=$(cat ${PROJECT}/.neuro/training-batch-params.yaml | envsubst | base64 -w 0)

      # Create a copy of pipeline template
      cp $${PROJECT}/config/pipeline.json /tmp/pipeline.json
      sed -i -e s/##NEURO_PASSED_CONFIG##/${NEURO_PASSED_CONFIG}/ /tmp/pipeline.json
      # Probagate project and batch definition and its parameters into the Pachyderm pypeline definition
      sed -i -e s/##PROJECT_B64##/${PROJECT_B64}/ -e s/##BATCH_B64##/${BATCH_B64}/ -e s/##PARAMS_B64##/${PARAMS_B64}/ /tmp/pipeline.json
      # Set the pipeline name
      sed -i -e s/##PIPELINE_NAME##/${PACHY_PIPELINE_NAME}/ /tmp/pipeline.json

      # Create pipeline
      pachctl list pipeline $PACHY_PIPELINE_NAME >/dev/null && pachctl delete pipeline $PACHY_PIPELINE_NAME >/dev/null
      pachctl create pipeline -f /tmp/pipeline.json
      echo "Pipeline '${PACHY_PIPELINE_NAME}' created successfully."

  prepare_remote_dataset:
    name: $[[ flow.title ]]-prepare-dataset
    image: neuromation/neuro-extras:20.12.15
    detach: False
    volumes:
      - ${{ volumes.remote_dataset.ref_rw }}
    params:
      dataset_path:
        default: http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar
        descr: URL of dataset to download
    bash: |
      DST=$(basename ${{ params.dataset_path }} .tar)
      if [ ! -e ${{ volumes.remote_dataset.mount }}/$DST ]; then
        neuro-extras data cp --extract --use-temp-dir ${{ params.dataset_path }} ${{ volumes.remote_dataset.mount }}/$DST;
      else
        echo "Dataset is already downloaded, skipping."
      fi

  extend_data:
    name: $[[ flow.title ]]-extend-data
    image: $[[ images.train.ref ]]
    detach: False
    volumes:
      - ${{ volumes.remote_dataset.ref_ro }}
      - secret:gh-rsa:/root/.ssh/id-rsa
    env:
      PROJECT: /usr/project
      PYTHONPATH: /usr/project
      PACHY_TOKEN: secret:pachyderm-token
    params:
      extend_dataset_by:
        default: "1"
        descr: |
          How many new images to add into current dataset.
    bash: |
      echo "IdentityFile ~/.ssh/id-rsa" > ~/.ssh/config
      ssh-keyscan github.com >> ~/.ssh/known_hosts
      git clone -b ${PROJECT_GIT_BRANCH} ${PROJECT_GIT_REPO} ${PROJECT}

      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl config set active-context default
      echo "${PACHY_TOKEN}" | pachctl auth use-auth-token

      export DATA_PATH=${PROJECT}/data/Images
      pachctl get file ${PACHY_REPO}@master:/data/Images -r -o ${DATA_PATH} | tee || true # if head is empty

      echo "Extending dataset"
      python ${PROJECT}/label_studio/extend_dataset.py \
        --cur_dataset ${DATA_PATH} \
        --full_dataset ${{ volumes.remote_dataset.mount }}/images/Images/ \
        --nmber_of_imgs ${{ params.extend_dataset_by }}

      # Push dataset
      pachctl put file -r ${PACHY_REPO}@master:data/Images/ -f ${DATA_PATH}/ | tee


      # Validate dataset:
      pachctl list commit ${PACHY_REPO}@master
      echo "Total images: $(pachctl list file ${PACHY_REPO}@master:data/Images | grep 'file' | wc -l)"

  label_studio:
    image: $[[ images.label_studio.ref ]]
    name: $[[ flow.title ]]-label-studio
    http_port: 443
    http_auth: False
    life_span: 1d
    detach: False
    browse: True
    volumes:
      - secret:gh-rsa:/root/.ssh/id-rsa
    env:
      PROJECT: /usr/project
      LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED: True
      LABEL_STUDIO_PROJECT: /label-studio-project
      PACHY_TOKEN: secret:pachyderm-token
      LS_TOKEN: secret:ls-token
      LS_PORT: 443
    bash: |
      echo "Downloading base dataset"
      echo "IdentityFile ~/.ssh/id-rsa" > ~/.ssh/config
      ssh-keyscan github.com >> ~/.ssh/known_hosts
      git clone -b ${PROJECT_GIT_BRANCH} ${PROJECT_GIT_REPO} ${PROJECT}

      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      echo "${PACHY_TOKEN}" | pachctl auth use-auth-token

      export DATA_PATH=${PROJECT}/data/Images
      COMPLETIONS=${LABEL_STUDIO_PROJECT}/completions
      # Pull dataset:
      pachctl get file ${PACHY_REPO}@master:/data/ -r -o ${PROJECT}/data/ | tee

      if [[ -f "${PROJECT}/data/label_studio.sqlite3" ]];
      then
        echo Restoring existing label studio data
        cp ${PROJECT}/data/label_studio.sqlite3 /label-studio/data/label_studio.sqlite3
      else
        echo Initializing label studio
        label-studio init dogs --username test@neu.ro --password 12345678 \
                --user-token ${LS_TOKEN}  \
                --label-config ${PROJECT}/label_studio/LabelConfig.xml
      fi


      echo "Starting label-studio"
      python3 ${PROJECT}/label_studio/launch_ls.py \
        --project_root ${PROJECT} \
        -- \
        start dogs --port ${LS_PORT} --no-browser

      # Push labeling completions (label studio-specific files), images and annotation results
      pachctl start commit ${PACHY_REPO}@master
      pachctl put file -r ${PACHY_REPO}@master:data/Images/ -f ${DATA_PATH} | tee
      pachctl put file    ${PACHY_REPO}@master:data/result.json -f ${PROJECT}/data/result.json | tee
      pachctl put file    ${PACHY_REPO}@master:data/label_studio.sqlite3 -f /label-studio/data/label_studio.sqlite3 | tee
      pachctl finish commit ${PACHY_REPO}@master -m "Commit annotations for `ls ${COMPLETIONS} | wc -l` images"

      # Validate dataset:
      pachctl list commit ${PACHY_REPO}@master
      echo "Total images: $(pachctl list file ${PACHY_REPO}@master:data/Images | grep 'file' | wc -l)"

  deploy_inference_platform:
    image: $[[ images.seldon.ref ]]
    name: $[[ flow.title ]]-test-inference
    preset: gpu-small
    http_port: 5000
    http_auth: False
    life_span: 5h
    detach: True
    params:
      run_id:
        descr: MLFlow run ID, which model should be deployed
      mlflow_storage:
        descr: Storage path, where MLFlow server stores model binaries
    volumes:
      - ${{ params.mlflow_storage }}/0/${{ params.run_id }}/artifacts/model/data/model.h5:/storage/model.h5

  locust:
    image: locustio/locust:1.4.1
    name: $[[ flow.title ]]-locust
    http_port: 8080
    http_auth: False
    life_span: 1d
    detach: True
    browse: True
    params:
      endpoint_url: 
        default: ~
        descr: |
          Examples:
          https://demo-oss-dogs-test-inference--<user-name>.jobs.<cluster-name>.org.neu.ro/api/v1.0/predictions - if model deployed as platform job
          http://seldon.<cluster-name>.org.neu.ro/seldon/seldon/<model-name>-<model-stage>/api/v1.0/predictions -if model is deployed in Seldon
    volumes:
      - $[[ upload(volumes.src).ref_ro ]]
      - $[[ upload(volumes.config).ref_ro ]]
      - $[[ volumes.remote_dataset.ref_ro ]]
    env:
      DOG_IDS: "n02085936, n02088094"
      IMGS_DIR: $[[ volumes.remote_dataset.mount ]]/images/Images/
      PYTHONPATH: $[[ volumes.src.mount ]]/..
    cmd: |
      -f $[[ volumes.src.mount ]]/locust.py --web-port 8080 -H $[[ params.endpoint_url ]]

  jupyter:                                                                     
    image: $[[ images.train.ref ]]
    name: $[[ flow.title ]]-jupyter
    preset: gpu-small
    http_port: 8888
    http_auth: False
    browse: True
    detach: False
    volumes:
      - "${{ upload(volumes.notebooks).ref_rw }}"
      - ${{ params.mlflow_storage }}/0/${{ params.run_id }}/artifacts/model/data/model.h5:/storage/model.h5
    env:
      PYTHONPATH: /usr/project
      PROJECT: /usr/project
      EXPOSE_SSH: "yes"
    params:
      run_id:
        descr: MLFlow run ID, which model should be deployed
      mlflow_storage:
        descr: Storage path, where MLFlow server stores model binaries
    bash: |
      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl inspect repo ${PACHY_REPO} || pachctl create repo ${PACHY_REPO}
      pachctl get file ${PACHY_REPO}@master:/data/Images -r ${PROJECT}/data/Images | tee

      jupyter notebook  \
        --no-browser \
        --ip=0.0.0.0 \
        --port=8888 \
        --allow-root \
        --NotebookApp.token= \
        --notebook-dir=${{ volumes.notebooks.mount }} \
        --NotebookApp.shutdown_no_activity_timeout=7200 \
        --MappingKernelManager.cull_idle_timeout=7200

# Additional jobs
  filebrowser:
    # https://neu-ro.gitbook.io/neuro-flow/reference/actions-syntax
    action: gh:neuro-actions/filebrowser@v1.0.1
    args:
      volumes_project_remote: storage:$[[ flow.flow_id ]]

  postgres:
    image: postgres:12.5
    name: $[[ flow.title ]]-postgres
    preset: cpu-small
    life_span: 30d
    detach: True
    volumes:
      - disk:mlops-demo-oss-dogs-postgres:/var/lib/postgresql/data:rw
    env:
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: ""
      PGDATA: /var/lib/postgresql/data/pgdata

  mlflow_server:
    image: neuromation/mlflow:1.11.0
    name: $[[ flow.title ]]-mlflow-server
    preset: cpu-small
    http_port: 5000
    http_auth: False
    browse: True
    life_span: 30d
    detach: True
    volumes:
      - storage:${{ flow.flow_id }}/mlruns:/usr/local/share/mlruns
    cmd: |
      server --host 0.0.0.0
        --backend-store-uri=postgresql://postgres:password@${{ inspect_job('postgres').internal_hostname_named }}:5432
        --default-artifact-root=/usr/local/share/mlruns
