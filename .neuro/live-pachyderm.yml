kind: live
title: demo-oss-dogs
id: mlops_demo_oss_dogs

jobs:
  prepare_remote_dataset:
    name: $[[ flow.title ]]-prepare-dataset
    image: neuromation/neuro-extras:20.12.15
    detach: False
    volumes:
      - ${{ volumes.remote_dataset.ref_rw }}
    params:
      dataset_path:
        default: http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar
        descr: URL of dataset to download
    bash: |
      DST=$(basename ${{ params.dataset_path }} .tar)
      if [ ! -e ${{ volumes.remote_dataset.mount }}/$DST ]; then
        neuro-extras data cp --extract --use-temp-dir ${{ params.dataset_path }} ${{ volumes.remote_dataset.mount }}/$DST;
      else
        echo "Dataset is already downloaded, skipping."
      fi

  extend_data:
    name: $[[ flow.title ]]-extend-data
    image: $[[ images.train.ref ]]
    detach: False
    pass_config: True
    volumes:
      - ${{ volumes.remote_dataset.ref_ro }}
      - ${{ upload(volumes.config).ref_ro }}
      - ${{ upload(volumes.label_studio).ref_ro }}
    env:
      PROJECT: /usr/project
      PYTHONPATH: /usr/project
    params:
      extend_dataset_by:
        default: "1"
        descr: |
          How many new images to add into current dataset.
    bash: |
      export DATA_PATH=${PROJECT}/data/Images
      mkdir -p ${DATA_PATH}

      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl config set active-context default
      pachctl get file ${PACHY_REPO}@master:/data/Images -r -o ${DATA_PATH} | tee || true # if head is empty

      echo "Extending dataset"
      python ${{ volumes.label_studio.mount }}/extend_dataset.py \
        --cur_dataset ${DATA_PATH} \
        --full_dataset ${{ volumes.remote_dataset.mount }}/images/Images/ \
        --nmber_of_imgs ${{ params.extend_dataset_by }}

      # Push dataset
      pachctl put file -r ${PACHY_REPO}@master:data/Images/ -f ${DATA_PATH}/ | tee

      # Validate dataset:
      pachctl list commit ${PACHY_REPO}@master
      echo "Total images: $(pachctl list file ${PACHY_REPO}@master:data/Images | grep 'file' | wc -l)"

  label_studio:
    image: $[[ images.label_studio.ref ]]
    name: $[[ flow.title ]]-label-studio
    http_port: 443
    http_auth: False
    life_span: 1d
    pass_config: True
    detach: False
    browse: True
    volumes:
      - ${{ upload(volumes.config).ref_ro }}
      - ${{ upload(volumes.label_studio).ref_ro }}
    env:
      PROJECT: /usr/project
      DATA_PATH: /usr/project/data/Images
      LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED: True
      LABEL_STUDIO_PROJECT: /label-studio-project
      LS_TOKEN: secret:ls-token
      LS_PORT: 443
    bash: |
      # Init pachctl and pull dataset
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl get file ${PACHY_REPO}@master:/data/ -r -o ${PROJECT}/data/ | tee

      COMPLETIONS=${LABEL_STUDIO_PROJECT}/completions
      mkdir -p ${COMPLETIONS}

      if [[ -f "${PROJECT}/data/label_studio.sqlite3" ]];
      then
        echo Restoring existing label studio data
        cp ${PROJECT}/data/label_studio.sqlite3 /label-studio/data/label_studio.sqlite3
      else
        echo Initializing label studio
        label-studio init dogs --username test@neu.ro --password 12345678 \
                --user-token ${LS_TOKEN}  \
                --label-config ${{ volumes.label_studio.mount }}/LabelConfig.xml
      fi


      echo "Starting label-studio"
      python3 ${{ volumes.label_studio.mount }}/launch_ls.py \
        --project_root ${PROJECT} \
        -- \
        start dogs --port ${LS_PORT} --no-browser


      # Push labeling completions (label studio-specific files), images and annotation results
      pachctl start commit ${PACHY_REPO}@master
      pachctl put file -r ${PACHY_REPO}@master:data/Images/ -f ${DATA_PATH} | tee
      pachctl put file    ${PACHY_REPO}@master:data/result.json -f ${PROJECT}/data/result.json | tee
      pachctl put file    ${PACHY_REPO}@master:data/label_studio.sqlite3 -f /label-studio/data/label_studio.sqlite3 | tee
      pachctl finish commit ${PACHY_REPO}@master -m "Commit annotations for `ls ${COMPLETIONS} | wc -l` images"

      # Validate dataset:
      pachctl list commit ${PACHY_REPO}@master
      echo "Total images: $(pachctl list file ${PACHY_REPO}@master:data/Images | grep 'file' | wc -l)"

  deploy_inference_platform:
    image: $[[ images.seldon.ref ]]
    name: $[[ flow.title ]]-test-inference
    preset: gpu-small
    http_port: 5000
    http_auth: False
    life_span: 5h
    detach: True
    params:
      run_id:
        descr: MLFlow run ID, which model should be deployed
      mlflow_storage:
        descr: Storage path, where MLFlow server stores model binaries
        default: 'storage:/${{ project.owner }}/${{ flow.id }}/mlruns'
    volumes:
      - ${{ params.mlflow_storage }}/0/${{ params.run_id }}/artifacts/model/data/model.h5:/storage/model.h5

  locust:
    image: locustio/locust:2.8.5
    name: $[[ flow.title ]]-locust
    http_port: 8080
    http_auth: False
    life_span: 1d
    detach: True
    browse: True
    params:
      endpoint_url:
        default: ~
        descr: |
          Examples:
          https://demo-oss-dogs-test-inference--<user-name>.jobs.<cluster-name>.org.neu.ro/api/v1.0/predictions - if model deployed as platform job
          http://seldon.<cluster-name>.org.neu.ro/seldon/seldon/<model-name>-<model-stage>/api/v1.0/predictions -if model is deployed in Seldon
    volumes:
      - $[[ upload(volumes.src).ref_ro ]]
      - $[[ upload(volumes.config).ref_ro ]]
      - $[[ volumes.remote_dataset.ref_ro ]]
    env:
      DOG_IDS: "n02085936, n02088094"
      IMGS_DIR: $[[ volumes.remote_dataset.mount ]]/images/Images/
      PYTHONPATH: $[[ volumes.src.mount ]]/..
    cmd: |
      -f $[[ volumes.src.mount ]]/locust.py --web-port 8080 -H $[[ params.endpoint_url ]]

  jupyter:
    image: $[[ images.train.ref ]]
    name: $[[ flow.title ]]-jupyter
    preset: gpu-small
    http_port: 8888
    http_auth: False
    browse: True
    detach: False
    volumes:
      - ${{ upload(volumes.notebooks).ref_rw }}
      - ${{ params.mlflow_storage }}/0/${{ params.run_id }}/artifacts/model/data/model.h5:/storage/model.h5
    env:
      PYTHONPATH: /usr/project
      PROJECT: /usr/project
      EXPOSE_SSH: "yes"
      DATA_PATH: /usr/project//data/Images
    params:
      run_id:
        descr: MLFlow run ID, which model should be deployed
      mlflow_storage:
        descr: Storage path, where MLFlow server stores model binaries
    bash: |
      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl inspect repo ${PACHY_REPO} || pachctl create repo ${PACHY_REPO}

      # Download data
      pachctl get file ${PACHY_REPO}@master:/data/Images -r ${PROJECT}/data/Images | tee

      jupyter notebook  \
        --no-browser \
        --ip=0.0.0.0 \
        --port=8888 \
        --allow-root \
        --NotebookApp.token= \
        --notebook-dir=${{ volumes.notebooks.mount }} \
        --NotebookApp.shutdown_no_activity_timeout=7200 \
        --MappingKernelManager.cull_idle_timeout=7200

  # Additional jobs
  filebrowser:
    # https://neu-ro.gitbook.io/neuro-flow/reference/actions-syntax
    action: gh:neuro-actions/filebrowser@v1.0.1
    args:
      volumes_project_remote: storage:$[[ flow.flow_id ]]

  postgres:
    image: postgres:12.5
    name: $[[ flow.title ]]-postgres
    preset: cpu-small
    life_span: 30d
    detach: True
    volumes:
      - disk:mlops-demo-oss-dogs-postgres:/var/lib/postgresql/data:rw
    env:
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: ""
      PGDATA: /var/lib/postgresql/data/pgdata

  mlflow_server:
    image: neuromation/mlflow:1.11.0
    name: $[[ flow.title ]]-mlflow-server
    preset: cpu-small
    http_port: 5000
    http_auth: False
    browse: True
    life_span: 30d
    detach: True
    volumes:
      - storage:${{ flow.flow_id }}/mlruns:/usr/local/share/mlruns
    cmd: |
      server --host 0.0.0.0
        --backend-store-uri=postgresql://postgres:password@${{ inspect_job('postgres').internal_hostname_named }}:5432
        --default-artifact-root=/usr/local/share/mlruns

  create_pipeline:
    name: $[[ flow.title ]]-create-pipeline
    image: $[[ images.train.ref ]]
    volumes:
      - ${{ upload(volumes.project).ref_ro }}
    params:
      pachy_pipeline_name:
        default: train-dogs
    env:
      EXPOSE_SSH: "yes"
      PYTHONPATH: /usr/project
      NEURO_PASSED_CONFIG: secret:platform-config
    bash: |
      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl config set active-context default
      pachctl inspect repo "${PACHY_REPO}" || \
          pachctl create repo "${PACHY_REPO}"

      # Store neuro-flow project definition in base64
      LIVE_B64=$(cat ${{ volumes.project.mount }}/.neuro/live.yml | base64 -w 0)
      PROJECT_B64=$(cat ${{ volumes.project.mount }}/.neuro/project.yml | base64 -w 0)

      # Create a copy of pipeline template
      cp ${{ volumes.project.mount }}/config/pipeline.json /tmp/pipeline.json

      # Set pipeline config parameters
      sed -i -e s/##NEURO_PASSED_CONFIG##/${NEURO_PASSED_CONFIG}/ /tmp/pipeline.json
      sed -i -e s/##PACHY_REPO##/${PACHY_REPO}/ /tmp/pipeline.json

      # Propagate project definition to the Pachyderm pipeline
      sed -i -e s/##PROJECT_B64##/${PROJECT_B64}/ -e s/##LIVE_B64##/${LIVE_B64}/ /tmp/pipeline.json

      # Set the pipeline name
      sed -i -e s/##PIPELINE_NAME##/${{ params.pachy_pipeline_name }}/ /tmp/pipeline.json

      # Create pipeline
      pachctl list pipeline ${{ params.pachy_pipeline_name }} >/dev/null && \
          pachctl delete pipeline ${{ params.pachy_pipeline_name }} >/dev/null
      pachctl create pipeline -f /tmp/pipeline.json

      echo "Pipeline '${{ params.pachy_pipeline_name }}' created successfully."

  train:
    image: $[[ images.train.ref ]]
    preset: cpu-medium
    life_span: 1d
    volumes:
      - storage:/${{ project.owner }}/${{ flow.flow_id }}/mlruns:/usr/local/share/mlruns
      - $[[ volumes.project.ref ]]
    env:
      EXPOSE_SSH: "yes"
      PYTHONPATH: /usr/project
      PROJECT: ${{ volumes.project.mount }}
    bash: |
      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl config set active-context default

      # Pull dataset:
      pachctl get file ${PACHY_REPO}@master:/data/ -r -o ${PROJECT}/data/ | tee

      python -u ${PROJECT}/src/train.py \
        --data_dir ${PROJECT}/data/Images \
        --data_description ${PROJECT}/data/result.json
